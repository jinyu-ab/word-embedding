# word-embedding

# 0.项目介绍
``词嵌入（word embedding）``是将词语映射到一个高维的向量空间中，其中每个词语通过一个稠密的向量表示，向量中的每个维度捕捉了词语的某种语义特征。

``Word2Vec``是一个常见的词嵌入方法，它通过神经网络模型来学习词语的向量表示。Word2Vec有两种常见的训练模式：Skip-Gram和CBOW（Continuous Bag of Words）。Skip-Gram模型通过给定一个单词预测其上下文词汇，而CBOW模型则是给定上下文词汇预测目标词。

``主成分分析（PCA，Principal Component Analysis）``是一种常用的降维技术，旨在通过线性变换将数据从高维空间投影到低维空间（通常是二维或三维），同时保留数据中的大部分变异信息。PCA的目的是找出数据中最重要的特征，减少数据的复杂性并提高后续分析的可视化效果。

本项目旨在通过使用金庸小说全集中的文本数据，基于pytorch训练一个Word2Vec模型，然后提取小说中人物角色的人名并将其嵌入到向量空间，使用PCA技术将这些人名的词向量降维到二维和三维空间，并利用Plotly进行交互式可视化展示，便于查看各个角色的相似度和分布。

# 1.数据集
项目选用了金庸小说全集，使用的主要数据集包括：
 - **jinyong_all_novel**：包含十五部金庸小说的 `.txt` 文件，这些文件包含了小说的完整文本。
- **jinyong_all_person.txt**：金庸小说中的所有人物名字列表，用于提取文本中的人物名字。

# 2.项目思路
0. **环境准备**：
   - 使用pip安装并导入相关库，包括torch, numpy, jieba, sklearn中的PCA，plotly，os，collections中的Counter等

1. **数据加载与预处理**：
   - 读取并清洗金庸小说文本数据，去除特殊符号和无意义字符。 
   - 使用jieba分词工具对文本进行分词处理，并去除停用词。

2. **构建训练数据集**：
   - 对于Skip-Gram模型，每个中心词生成对应的上下文词。窗口大小设为5左右，左右各取n个词。例如，对于每个中心词的位置i，取i-5到i+5之间的词，排除自身，作为上下文。然后生成（中心词，上下文词）对。负采样的话，可能需要对每个正样本生成k个负样本。

3. **训练Word2Vec模型**：
   - 使用PyTorch实现Word2Vec模型，采用Skip-Gram方法对词汇进行嵌入训练。
   - 利用训练好的词向量表示金庸小说中的每个词语。

4. **提取人物向量和PCA降维**：
   - 提取小说中的人物名字，并获取这些人名对应的词向量。
   - 使用PCA将人物角色的词向量降维到2D和3D空间。
   
5. **交互式可视化**：
   - 使用Plotly进行交互式可视化展示，使得用户可以通过鼠标悬停查看具体人物的信息。
   - 通过二维和三维可视化展示人物词向量之间的关系和分布，帮助理解小说人物间的语义相似性。

# 3.目录结构描述
- **ReadMe.md**：本项目的帮助文档，包含项目介绍、数据集介绍、项目思路、目录结构、环境依赖等。
- **jinyong_all_novel**：本项目使用的数据集，包含十五部金庸小说的 `.txt` 文件，这些文件包含了小说的完整文本。
- **jinyong_all_person.txt**：金庸小说中的所有人物名字列表，用于提取文本中的人物名字。
- **stopwords.txt**：本项目使用的停用词表，包含一些常见的停用词。
- **word_embedding.ipynb**：本项目的完整代码文件，使用Jupyter Notebook打开，包含数据加载、预处理、训练Word2Vec模型、提取人物向量、PCA降维和可视化等步骤。

# 4.环境依赖
* Python 3.11.8
* pytorch 2.2.1
* numpy
* jieba 
* sklearn
* plotly

# 5.结论
通过本项目，我们展示了如何利用金庸小说全集训练一个``Word2Vec``模型，并将小说中的人物角色嵌入到向量空间中。通过``PCA降维``和``交互式可视化``，我们能够直观地查看不同人物之间的语义相似度和分布情况。

这种方法不仅能够帮助我们理解小说人物间的关系，还可以应用于其他类似的文本分析任务，如情感分析、主题建模等。通过本项目，我们进一步验证了词嵌入技术和降维技术在自然语言处理中的有效性。


